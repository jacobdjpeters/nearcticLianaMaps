---
title: "gbifDataCollection"
author: "Jacob Peters"
date: '2022-10-30'
output: html_document
---


This script is for downloading presence points from GBIF and writing them to shapefiles. 

This script is different because it is for thinning only. 
This script does not clip data to forest. It simply thins to 999 points. 


```{r packages and working directory, echo=FALSE, include=FALSE, results=FALSE}

install.packages(c("readxl", "ggplot2", "knitr", "data.table", "randomForest", 
                 "randomForestExplainer", "randomForestSRC", "raster", "rgdal", 
                 "rgbif", "gbifdb","sp", "sf", "gridExtra", "grid", "pdp", "vip", 
                 "stars", "ggmap", "maptools", "mapdata", "viridis", "rgeos", 
                 "CENFA", "dismo", "sdmvspecies",
                 "RasterVis", "terra", "CoordinateCleaner", "countrycode"))


```

#Load Packages and set params

```{r setup,  echo=FALSE, include=FALSE}

require(readxl)
require(dismo)
require(ggplot2)
theme_set(theme_bw())
require(knitr)
require(data.table)
require(randomForest)
require(randomForestExplainer)
require(randomForestSRC)
require(raster)
require(terra)
require(rgdal)
require(sp)
require(sf)
require(stars)
require(gridExtra)
require(grid)
require(pdp)
require(vip)
require(ggmap)
require(maptools)
require(mapdata)
require(viridis)
require(rgeos)
require(CENFA)
require(sdmvspecies)
require(rasterVis)
require(ROCR)
require(rgbif)
require(CoordinateCleaner)
require(countrycode)
opts_chunk$set(fig.align = 'center')

memory.limit()
memory.limit(size=1800)



# params - run first


# or load below
northAmerica <- vect("/home/jacob/SpatialData/geography/NorthAmerica/US_Can_Mex_Boundary_noAttu_4326.shp")



# load north america us and canada
uscan <- vect("/home/jacob/SpatialData/geography/NorthAmerica/US_Can_Boundary_noAttu_4326.shp")

# load nearctic
nearctic <- vect("/home/jacob/SpatialData/ecoregion/ecoRegions2017_nearcticRealm.shp")



# for grabbing columns 
columnNames <- c("species", "specificEpithet", "decimalLongitude", "decimalLatitude", "countryCode",
             "individualCount", "gbifID", "family", "taxonRank",
             "coordinateUncertaintyInMeters", "year", "basisOfRecord", 
             "institutionCode", "datasetName")

# for maps

terraOptions(tempdir="/home/jacob/RStuff/tempdir")
terraOptions()
forest100 <- rast("/home/jacob/SpatialData/land_cover/NA/naForest_0100m_epsg4326.tif")
forest100 <- mask(forest100, nearctic) # mask to roi
#forestMask <- forest100[forest100==0] <- NA

```


# FUNCTION: Master. Download, clean, thin. 


Note that thinning before clipping to forest is silly. 
It removes a lot of data that it probably shouldn't. 
If you want to clip to forest, do that, THEN thin them. 


Have this thing do the following: 
  1. download and clean data, write shapefiles.
  2. spatially thin data, write shapefiles. 
  
 
  

```{r}


GBIFDataPrep <- function(speciesName, # species name for searching GBIF. 
                         familyName, # familyName for cleaning data.
                         country="US", # to supply geographic boundary for search. Default is "US"
                         date, #date. character 
                         roi, #for setting crs of presence points. 
                         maskLayer = NULL, # if masking, specify raster to mask to. 
                         columnNames=c("species", 
                                       "specificEpithet", 
                                       "decimalLongitude", 
                                       "decimalLatitude",
                                       "countryCode",
                                       #"individualCount", 
                                       "gbifID", 
                                       "family", 
                                       "taxonRank",
                                       "coordinateUncertaintyInMeters", 
                                       "year", 
                                       "basisOfRecord",
                                       "institutionCode", 
                                       "datasetName"), # what columns we want from the GBIF record. Feel free to modify. 
                         geometry=NULL, # NULL by default. See occ_search for info there. 
                         folderPath, # file path for folder where files will be written. 
                         nthin, #number of points to thin to. 
                         name # the unique part of the file name that you want for the taxon, without extension (those are added automatically)   ## note that the other filename parts like "InForest" will be added automatically as well. 
                         ){


#obtain data from GBIF via rgbif
    datList <- list(NULL) # make list for storing data
    k <- 1 # counter for storing data frames in datList. 
    for(i1 in country){ # loop through country list
      print(paste("for", i1, sep=" ")) # track it.
    dat <- occ_search(scientificName = speciesName, limit = 10000, hasCoordinate = T,
                      country=i1,
                      geometry=geometry)
    if(dat[[1]]$count > 0){
    data <- as.data.table(dat$data)
    data <- data[, ..columnNames]
    #convert country code from ISO2c to ISO3c
    data$countryCode <-  countrycode(data$countryCode, origin =  'iso2c', destination = 'iso3c')
    data <- data[!is.na(decimalLongitude) & !is.na(decimalLatitude)]
    
    flags <- clean_coordinates(x = data, 
                               lon = "decimalLongitude", 
                               lat = "decimalLatitude",
                               countries = "countryCode",
                               species = "species",
                               tests = c("capitals", 
                                         "centroids", 
                                         "equal",
                                         "gbif", 
                                         "institutions",
                                         "zeros")) # most test are on by default
    
    #Exclude problematic records
    data$flags <- flags$.summary
    dat_cl <-  data[flags!=FALSE]
    datList[[k]] <- dat_cl
    }
    k <- k+1
}
    dat_cl <- do.call("rbind", datList) # merge data frames. 
    
    # select accuracy of points. no more than 30m error.. 
    dat_cl <- dat_cl[coordinateUncertaintyInMeters <= 30] # get only best accuracy. 
                     
    dat_cl <- dat_cl[basisOfRecord == "HUMAN_OBSERVATION" | 
                             basisOfRecord == "OBSERVATION"]
    # weird counts
    #dat_cl <- dat_cl[individualCount >= 0 | is.na(individualCount)] 
    # temporal outliers. remove old stuff. 
    dat_cl <- dat_cl[year >= 1945]
    # make sure no incorrect family ID. 
    dat_cl <- dat_cl[family==familyName]
    # get coords, make spdf.
    coords <- cbind(dat_cl$decimalLongitude,
                    dat_cl$decimalLatitude) # get coords of clean data. 
    pres <- SpatialPointsDataFrame(coords, data=dat_cl, proj4string = CRS("+init=epsg:4326")) # make spdf
    pres <- spTransform(pres, crs(roi))
    pres <- vect(pres) #convert to spatVector
    crs(pres) <- crs(roi) # specify crs. 
    pres <- crop(pres, roi) # crop to your ROI 
    print(paste0("crs check: ", crs(pres)))
    #writeVector(pres,
    #        paste0(folderPath, name, "Clean_", type, "_", date, ".shp"),
    #        overwrite=TRUE) # if you want to write a file without thinning or masking. 
    

# mask if needed
  if(!is.null(maskLayer)){
      print("masking to maskLayer... ")
  values(pres) <- extract(maskLayer, pres) # extract mask values
  pres <- pres[pres$discrete_classification==1] # select where land cover ==1 (forest)
  
    }




# Prepare to thin presence

    pres$random <- runif(length(pres)) # add random numbers to all original presence data. 
    pres <- pres[order(pres$random)] # randomly sort. 
    pres$id <- 1:length(pres) #assign a sequential ID. This will match "from" and "to" below. 

    
    # some of them have way too many points. 
    # if greater than 40k points, randomly thin to 30k. 
    if(length(pres)>40000){
      print(paste("There are", length(pres), "points. Randomly thinning to 30k."))
      pres<- pres[pres$id<=30000]
      print(paste("thinned to", length(pres), "points."))
    }


    # for all taxa: thin points within 1km of another reduces sampling bias. 
    presPrj <- as(pres, "Spatial") # convert to sp
    presPrj <- spTransform(presPrj, CRS("+init=epsg:5070")) # reproject to an equal area projection in meters. I think this helps the dist function run faster. 
    presPrj <- vect(presPrj) # put back as spatVector. 

    
# create reference table for thinning operation. gc() after each function because it is a hoss. 
    print("calculating distances ...")
		dTable <- data.table(terra::distance(presPrj, pairs=TRUE, symmetrical=TRUE)); gc() # clean garbage after. 
		dTable <- dTable[value<=50000000]; gc() # remove far distance pairs. 		
		head(dTable[from==6]); gc() # it does not count itself when calculating distances. 
		dTable <- dTable[order(value), head(value, n=25), by=.(from)]; gc() # get top 25 closest for each point. 
		head(dTable)
		colnames(dTable) <- c("id", "dist") # returns ID and distances. 
		dTable$dist <- as.numeric(dTable$dist)#/1000 # convert to km for easy reading. 
    gc() # collect garbage.  
    
    presRef <- cbind(pres$id, pres$random)
    colnames(presRef) <- c("id", "random")
    dTable <- merge(dTable, presRef, by="id", all.x=FALSE, all.y=FALSE)
    
    pointsToRemove <- sort(unique(dTable[dist<=15]$random)) # make list of points to thin. 

# randomly thin points so no point has another within 15m of it. 
    print("thinning to 15m to reduce sample bias ...")
    pres <- pres[! pres$random %in% pointsToRemove, ] # thin. 
    dTable <- dTable[! random %in% pointsToRemove, ] # remove from distance ref table.  



# if there are more than 'nthin' presence points, begin thinning. 
if(length(pres)>=nthin){

nToRemove <- length(pres) - nthin 
print(paste("thinning", nToRemove, "points from", name))

    

for(range in seq(30, 50000, by=15)){ # meters already thinned slightly for sampling bias.  
    
    if(any(dTable$dist<range)){ # if any distance values are less than given m range. 

      print(paste("thinning at", range, "m", sep=" "))
      
      if(length(unique(dTable[dist<=range]$random))<nToRemove){ # if there are fewer points to thin at this range than are needed to be removed, thin all pairs. 
        print("going with: thin all pairs")
      pointsToRemove <- sort(unique(dTable[dist<=range]$random)) # make list of points to thin. 
      
      pres <- pres[! pres$random %in% pointsToRemove, ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove, ] # remove from distance table.  
      
      nToRemove <- length(pres) - nthin  # update for remaining nToRemove. 
      print(paste("thinned", length(pointsToRemove), "points. ", length(pres), "remain."))
      }else{ # else, remove as many as we can
        print("going with: else")
      pointsToRemove <- sort(unique(dTable[dist<=range]$random)) # make list of points to thin.

      
      pres <- pres[! pres$random %in% pointsToRemove[1:nToRemove], ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove[1:nToRemove], ] # remove from distance table.   
      
      print(paste("thinned", nToRemove, "points", length(pres), "remain."))
      # I think once this else part runs, it should be done. 
        
      }
        
      if(length(pres)<=nthin){break} # if we reach goal, break.
        
      }
        
  if(length(pres)<=nthin){break} # if we reach goal, break. 
    
  }
}

if(is.null(maskLayer)){presType <- "CleanThin"}
    else{
      presType <- "CleanThinMasked"
}

  
  writeVector(pres,
        paste0(folderPath, name, "_", presType, "_", date, ".shp"),
        overwrite=TRUE)




}




```







# LOOP: GBIF download, clean, thin, clip

  Note that this doesn't work for vispShort, because we had to go through and select specific species. 
  
  This function is specifically for getting data, masking it to forest, and rasterizing at resolution specified by the "rasterRef" parameter. 

```{r}


# upate as of october 2024: take out honeysuckle. add virginia creeper and poison ivy. 

# triple check make sure these are aligned. 
lianas <- c("Vitis*"
            ,"Pueraria montana"
            ,"Celastrus orbiculatus"
            ,"Lonicera japonica"
            ,"Wisteria*"
            ,"Parthenocissus quinquefolia"
            ,"Campsis radicans"
            ,"Toxicodendron radicans"
            ,"Hedera helix"
            )
famNames <- c("Vitaceae"
              ,"Fabaceae"
              ,"Celastraceae"
              ,"Caprifoliaceae"
              ,"Fabaceae"
              ,"Vitaceae"
              ,"Bignoniaceae"
              ,"Anacardiaceae"
              ,"Araliaceae"
              )
names <- c("visp"
           ,"pumo"
           ,"ceor"
           ,"loja"
           ,"wisp"
           ,"paqu"
           ,"cara"
           ,"tora"
           ,"hehe"
           )


# download from GBIF
for(i in 1:length(lianas)){
  
    print(paste("starting", lianas[i], sep=" ")) # print progress
  
    GBIFDataPrep(speciesName=lianas[i],
                 familyName=famNames[i],
                 country=c("US", "CA", "MX"),
                 maskLayer = forest100,
                 date="20240417",
                 roi=nearctic, # for setting crs of points
                 folderPath="/home/jacob/_Research/lianaDist/dataPrep/spatial/lianaPres/northAmerica/",
                 nthin=999, # number to thin to. 
                 name=names[i])
    gc()
}





```





# download specific lianas as needed.

```{r}

 GBIFDataPrep(speciesName="Hedera helix",
              familyName="Araliaceae", # should be capitalized! 
              hsm=TRUE,
              date="YYYYMMDD",
              roi=usa,
              folderPath="/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/",
              nthin = 999,
              name="hehe")

```




# FUNCTION: thin presence (skip GBIF download)

This function below will load Cleaned presence points and basically do what the rest of the GBIF downloader would do (thin to reduce sample bias, clip to forest). 


```{r}



presThinner <- function(roi, #for setting crs of presence points. 
                        folderPath, # file path for folder where files will be written. 
                        type, #"hsm" or "sdm" ?
                        date, #character. when did data download happen?
                        nthin, #number of points to thin to. 
                        name # the unique part of the file name that you want for the taxon, without extension (those are added automatically)   ## note that the other filename parts like "InForest" will be added automatically as well. 
                        ){

pres <- vect(paste0(folderPath, name, "CleanThin", type, date, ".shp"))  # load cleaned data. 
  
  
# Prepare to thin presence

    pres$random <- runif(length(pres)) # add random numbers to all original presence data. 
    pres <- pres[order(pres$random)] # randomly sort. 
    pres$id <- 1:length(pres) #assign a sequential ID. This will match "from" and "to" below. 
    
    
    
    # some of them have way too many points. 
    # if greater than 40k points, randomly thin to 30k. 
    if(length(pres)>40000){
      print(paste("There are", length(pres), "points. Randomly thinning to 30k."))
      pres<- pres[pres$id<=30000]
      print(paste("thinned to", length(pres), "points."))
    }


    # for all taxa: thin points within 1km of another reduces sampling bias. 
    presPrj <- as(pres, "Spatial") # convert to sp
    presPrj <- spTransform(presPrj, CRS("+init=epsg:5070")) # reproject to an equal area projection in meters. I think this helps the dist function run faster. 
    presPrj <- vect(presPrj) # put back as spatVector. 
    

    
    
# create reference table for thinning operation. gc() after each function because it is a hoss. 
    print("calculating distances ...")
		dTable <- data.table(terra::distance(presPrj, pairs=TRUE, symmetrical=TRUE)); gc() # clean garbage after. 
		dTable <- dTable[value<=50000000]; gc() # remove far distance pairs. 		
		head(dTable[from==6]); gc() # it does not count itself when calculating distances. 
		dTable <- dTable[order(value), head(value, n=25), by=.(from)]; gc() # get top 25 closest for each point. 
		head(dTable)
		colnames(dTable) <- c("id", "dist") # returns ID and distances. 
		dTable$dist <- as.numeric(dTable$dist)#/1000 # convert to km for easy reading. 
    gc() # collect garbage.  
    
    presRef <- cbind(pres$id, pres$random)
    colnames(presRef) <- c("id", "random")
    dTable <- merge(dTable, presRef, by="id", all.x=FALSE, all.y=FALSE)
    
    pointsToRemove <- sort(unique(dTable[dist<=15]$random)) # make list of points to thin. 

# randomly thin points so no point has another within 15m of it. 
    print("thinning to 15m to reduce sample bias ...")
    pres <- pres[! pres$random %in% pointsToRemove, ] # thin. 
    dTable <- dTable[! random %in% pointsToRemove, ] # remove from distance ref table.  



# if there are more than 'nthin' presence points, begin thinning. 
if(length(pres)>=nthin){

nToRemove <- length(pres) - nthin 
print(paste("thinning", nToRemove, "points from", name))

    

for(range in seq(30, 50000, by=15)){ # meters already thinned slightly for sampling bias.  
    
    if(any(dTable$dist<range)){ # if any distance values are less than given m range. 

      print(paste("thinning at", range, "m", sep=" "))
      
      if(length(unique(dTable[dist<=range]$random))<nToRemove){ # if there are fewer points to thin at this range than are needed to be removed, thin all pairs. 
        print("going with: thin all pairs")
      pointsToRemove <- sort(unique(dTable[dist<=range]$random)) # make list of points to thin. 
      
      pres <- pres[! pres$random %in% pointsToRemove, ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove, ] # remove from distance table.  
      
      nToRemove <- length(pres) - nthin  # update for remaining nToRemove. 
      print(paste("thinned", length(pointsToRemove), "points. ", length(pres), "remain."))
      }else{ # else, remove as many as we can
        print("going with: else")
      pointsToRemove <- sort(unique(dTable[dist<=range]$random)) # make list of points to thin.

      
      pres <- pres[! pres$random %in% pointsToRemove[1:nToRemove], ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove[1:nToRemove], ] # remove from distance table.   
      
      print(paste("thinned", nToRemove, "points", length(pres), "remain."))
      # I think once this else part runs, it should be done. 
        
      }
        
      if(length(pres)<=nthin){break} # if we reach goal, break.
        
      }
        
  if(length(pres)<=nthin){break} # if we reach goal, break. 
    
  }
}

writeVector(pres,
        paste0(folderPath, name, "CleanThin_20240213.shp"),
        overwrite=TRUE)



}




```





# LOOP: skip GBIF download. thin, clip

I should tweak these functions to write a text file or something that details how much thinning occurred for each species. 

Do that at some point. 


```{r}


names <- c("pumo",
           "visp",
           "ceor",
           "loja",
           "wisp",
           "paqu",
           "cara",
           "tora",
           "hehe")
folderPath <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"



# run pres thinner and stuff
for(i in 1:length(names)){
  
    print(paste("starting", names[i], sep=" ")) # print progress
  
    presThinner(folderPath=folderPath,
                nthin=999, # number to thin to. 
                name=names[i])
    gc()
}




```


# thin specific lianas as needed. 

```{r}



 presThinner(folderPath="/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/",
             thin = 999,
             name="tora",
             maskLayer=forest30)


```

# make "all lianas" file. 

```{r}
# combine all lianas into one shapefile, no thinning, no clipping to forest. 

lianas <- c("pumoClean",
            "vispClean",
            "ceorClean",
            "lojaClean",
            "wispClean",
            "paquClean",
            "caraClean",
            "toraClean")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"

fileNames <- paste0(path, lianas, ".shp")

print(paste0("######### for point shapefiles #########"))
pres <- vect(fileNames[1]) # get first one
for(i in 2:length(lianas)){
  
  print(lianas[i])
  pres <- rbind(pres, vect(fileNames[i]))
  print(length(pres))
  
}

plot(pres)

writeVector(pres, "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/allLCleanNoThin.shp",
            overwrite=TRUE)



############# now combine thinned data, and thin that. 

# thin All Lianas. 

lianas <- c("pumoCleanThin",
            "vispCleanThin",
            "ceorCleanThin",
            "lojaCleanThin",
            "wispCleanThin",
            "paquCleanThin",
            "caraCleanThin",
            "toracleanThin")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"

fileNames <- paste0(path, lianas, ".shp")
pres <- vect(fileNames[1])

for(i in 2:length(fileNames)){
  
  pres <- rbind(pres, vect(fileNames[i]))
  print(paste0(length(pres), "points"))
  
}

plot(pres)

# thinned shapefiles combined: 
writeVector(pres, "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/allLClean.shp",
            overwrite=TRUE)

# run pres thinner 
gc()
folderPath <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"


presThinner(folderPath=folderPath,
            nthin=999, # number to thin to. 
            name="allL")
gc()









```




# rasterize pres data

```{r rasterize points}

# rasterize at different resolutions


masks <- list(forestMask1000, forestMask100, forestMask30)
forests <- list(forest1000, forest100, forest30)
resolutions <- c(1000
                 , 100
                 , 30
                 )
for(j in 1:3){
  mask <- masks[[j]]
  forest <- forests[[j]]
  resolution <- resolutions[j]
  
  for(i in 1:length(lianas)){
    print(paste("starting", resolution, "m", lianas[i], sep=" ")) # print progress
    
    rasterizePresence(
      forest=forest,
      forestMask=mask, # for cropping and masking presence data to forest, determining crs, rasterizing, and determining resolution.
      maskPath=paste0("/home/jacob/SpatialData/land_cover/USA/usaForestMASK_", resolution, "m_epsg4326.tif"),
      folderPath="/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/",
      rasterResolution=resolution, # for filenames
      name=fileNames[i])
}
}

# this has been tested, it works. 


```



# get npres 

```{r}

### for shapefiles. 

lianas <- c("pumoClean",
            "vispClean",
            "ceorClean",
            "lojaClean",
            "wispClean",
            "paquClean",
            "caraClean",
            "toraclean")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"

fileNames <- paste0(path, lianas, ".shp")

print(paste0("######### for point shapefiles #########"))
for(i in seq_along(lianas)){
  
  print(lianas[i])
  pres <- vect(fileNames[i])
  print(length(pres))
  
}




```



# Make "sampling density map" 

Perhaps here we combine data for all lianas that we have downloaded. 

Combine point files together and make a sampling density map. 

This doesn't work, for now. 


```{r}
usa <- vect("/home/jacob/SpatialData/geography/USA/usaBoundary4326.shp")

### for shapefiles. 

lianas <- c("pumoClean",
            "vispClean",
            "ceorClean",
            "lojaClean",
            "wispClean",
            "paquClean",
            "caraClean",
            "toraclean")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"
fileNames <- paste0(path, lianas, ".shp")
allPres <- crop(vect(fileNames[1]), usa) # load first. 

for(i in 2:length(fileNames)){
  
  pres <- crop(vect(fileNames[i]), usa)
  allPres <- rbind(allPres, pres)
  
}

length(allPres)


r <- forest100
plot(r)
x <- rasterize(allPres, r, fun=sum)
plot(x)


```



# testing


```{r}

  #obtain data from GBIF via rgbif
dat <- occ_search(scientificName = "Lonicera japonica", limit = 200000, hasCoordinate = T,
                  country="US",
                  geometry=NULL)
data <- as.data.table(dat$data)
data <- data[, ..columnNames]
#convert country code from ISO2c to ISO3c
data$countryCode <-  countrycode(data$countryCode, origin =  'iso2c', destination = 'iso3c')
data <- data[!is.na(decimalLongitude) & !is.na(decimalLatitude)]

flags <- clean_coordinates(x = data, 
                           lon = "decimalLongitude", 
                           lat = "decimalLatitude",
                           countries = "countryCode",
                           species = "species",
                           tests = c("capitals", 
                                     "centroids", 
                                     "equal",
                                     "gbif", 
                                     "institutions",
                                     "zeros")) # most test are on by default

#Exclude problematic records
data$flags <- flags$.summary
dat_cl <-  data[flags!=FALSE]
# if lsited, get data with good certainty on GPS. <1km. 
dat_cl <- dat_cl[coordinateUncertaintyInMeters <= 1000 |
                         is.na(coordinateUncertaintyInMeters)]
dat_cl <- dat_cl[basisOfRecord == "HUMAN_OBSERVATION" | 
                         basisOfRecord == "OBSERVATION"]
# weird counts
dat_cl <- dat_cl[individualCount >= 0 | is.na(individualCount)] 
# temporal outliers. remove old stuff. 
dat_cl <- dat_cl[year >= 1945]
# make sure no incorrect family ID. 
dat_cl <- dat_cl[family=="Fabaceae"]
# get coords, make spdf.
coords <- cbind(dat_cl$decimalLongitude,
                dat_cl$decimalLatitude) # get coords of clean data. 
pres <- SpatialPointsDataFrame(coords, data=dat_cl, proj4string = CRS("+init=epsg:4326")) # make spdf
pres <- spTransform(pres, crs(roi))
pres <- vect(pres) #convert to spatVector
crs(pres) <- crs(roi) # specify crs. 
pres <- crop(pres, roi) # crop to your ROI (usa, most likely)
print(paste0("crs check: ", crs(pres)))
writeVector(pres,
        paste0(folderPath, name, "Clean.shp"),
        overwrite=TRUE)



```


### different way

Try different way 

Loop through points randomly, and remove them if they are within Xkm from another point. 
Do this for 1...32 km as above, stopping when there are 5000 points. 

This is taking forever though 


```{r}
rm(r)
usa <- vect("/home/jacob/SpatialData/geography/USA/usaBoundary4326.shp")

### for shapefiles. 

lianas <- c("pumoClean",
            "vispClean",
            "ceorClean",
            "lojaClean",
            "wispClean",
            "paquClean",
            "caraClean",
            "toraclean")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"
fileNames <- paste0(path, lianas, ".shp")

folderPath <- path

pres <- crop(vect(fileNames[5]), usa) 
plot(pres)
pres$random <- runif(length(pres)) # add random numbers to all original presence data. 
head(pres$random) # check random
pres <- sort(pres, )
pres$id <- 1:length(pres) #assign a sequential ID. This will match "from" and "to" below. 
head(pres$id) # check id




# if there are more than 5000 presence points, begin thinning. 
if(length(pres)>1000){
print ("thinning for XXX")  
  
presPrj <- as(pres, "Spatial") # convert to sp
presPrj <- spTransform(presPrj, CRS("+init=epsg:5070")) # reproject to an equal area projection in meters. I think this helps the dist function run faster. 
presPrj <- vect(presPrj) # put back as spatVector. 

# create reference table for thinning operation. gc() after each function because it is a hoss. 
    print("calculating distances ...")
		dTable <- data.table(terra::distance(presPrj, pairs=TRUE, symmetrical=TRUE)); gc() # clean garbage after. 
		dTable <- dTable[value<=32000]; gc() # remove far distance pairs. 		
		head(dTable[from==6]); gc() # it does not count itself when calculating distances. 
		dTable <- dTable[order(value), head(value, n=25), by=.(from)]; gc() # get top 25 closest for each point. 
		head(dTable)
		colnames(dTable) <- c("id", "dist") # returns ID and distances. 
		dTable$dist <- as.numeric(dTable$dist)/1000 # convert to km for easy reading. 
    gc() # collect garbage.  
    presRef <- cbind(pres$id, pres$random)
    colnames(presRef) <- c("id", "random")
    dTable <- merge(dTable, presRef, by="id", all.x=FALSE, all.y=FALSE)
    

while(length(pres)>1000){ # while pres retains more than 5000 presence points: 
  
  for(range in 1:32){ # kilometers.
    
    while(any(dTable$dist<range)==TRUE){ # while any distance values are less than chosen km range. 
      k <- 0
      print(paste("thinning at", range, "km", sep=" "))
      for(i in sort(unique(dTable[dist<=range]$random))){ # grab random points in that km range.  
        
        dTable <- dTable[random!=i]
        pres <- pres[pres$random!=i]
        k <- k+1
        
        if(length(pres)<1000){break} # if we reach goal of 5000 pres max, break.
      }
        print(paste("removed", k, "points at", range, "km range", sep= " "))
        if(length(pres)<1000){break} # if we reach goal of 5000 pres max, break. 
    }
        if(length(pres)<1000){break} # if we reach goal of 5000 pres max, break. 
  }
        if(length(pres)<1000){break} # if we reach goal of 5000 pres max, break. 
}
}
    
name<-"wispClean"
writeVector(pres,
        paste0(folderPath, name, "Thin.shp"),
        overwrite=TRUE)
    



##### Now we have a list of points that are within 32km of another point.
##### we have random ID's that link this table of distance to pres points in shapefile.  
##### now we cycle through randomID's for certain distances (starting with <1km) 
##### and randomly delete those from the pres shapefile based on their 'random' ID. 




```


# third way? 

Would it be easier to run it and say " I need to remove X number of points." then do so randomly?

This would be an alternative to constantly removing one point at a time and then checking if we've hit the goal. 

Just ask "how many do we need to thin?"  "if we thin all of these at X range, will we still need to thin?"   if so, thin them all (randomly, just skip the loop I've been doing)

```{r}

usa <- vect("/home/jacob/SpatialData/geography/USA/usaBoundary4326.shp")

### for shapefiles. 

lianas <- c("pumoClean",
            "vispClean",
            "ceorClean",
            "lojaClean",
            "wispClean",
            "paquClean",
            "caraClean",
            "toraclean")
path <- "/home/jacob/_Research/lianaDist/USA/spatial/lianaPres/"
fileNames <- paste0(path, lianas, ".shp")

folderPath <- path

pres <- crop(vect(fileNames[5]), usa)
plot(pres)
pres$random <- runif(length(pres)) # add random numbers to all original presence data. 
head(pres$random) # check random
pres$id <- 1:length(pres) #assign a sequential ID. This will match "from" and "to" below. 
head(pres$id) # check id
nthin <- 999
name <- "wisp"






# if there are more than nthin presence points, begin thinning. 
if(length(pres)>=nthin){

nToRemove <- length(pres) - nthin 
print(paste("thinning", nToRemove, "points from", name))
  
presPrj <- as(pres, "Spatial") # convert to sp
presPrj <- spTransform(presPrj, CRS("+init=epsg:5070")) # reproject to an equal area projection in meters. I think this helps the dist function run faster. 
presPrj <- vect(presPrj) # put back as spatVector. 

# create reference table for thinning operation. gc() after each function because it is a hoss. 
    print("calculating distances ...")
		dTable <- data.table(terra::distance(presPrj, pairs=TRUE, symmetrical=TRUE)); gc() # clean garbage after. 
		dTable <- dTable[value<=32000]; gc() # remove far distance pairs. 		
		head(dTable[from==6]); gc() # it does not count itself when calculating distances. 
		dTable <- dTable[order(value), head(value, n=25), by=.(from)]; gc() # get top 25 closest for each point. 
		head(dTable)
		colnames(dTable) <- c("id", "dist") # returns ID and distances. 
		dTable$dist <- as.numeric(dTable$dist)/1000 # convert to km for easy reading. 
    gc() # collect garbage.  
    
    presRef <- cbind(pres$id, pres$random)
    colnames(presRef) <- c("id", "random")
    dTable <- merge(dTable, presRef, by="id", all.x=FALSE, all.y=FALSE)
    

for(range in 1:32){ # kilometers.
    
    if(any(dTable$dist<range)){ # if any distance values are less than chosen km range. 

      print(paste("thinning at", range, "km", sep=" "))
      
      if(length(unique(dTable[dist<=range]$random))<nToRemove){ # if there are fewer points to thin at this range than are needed to be removed, thin all pairs. 
        print("going with: thin all pairs")
      pointsToRemove <- unique(dTable[dist<=range]$random) # make list of points to thin. 
      
      pres <- pres[! pres$random %in% pointsToRemove, ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove, ] # remove from distance table.  
      
      nToRemove <- length(pres) - nthin  # update for remaining nToRemove. 
      print(paste("thinned", length(pointsToRemove), "points. ", length(pres), "remain."))
      }else{ # else, remove as many as we can
        print("going with: else")
      pointsToRemove <- sort(unique(dTable[dist<=range]$random)) # make list of points to thin.

      
      pres <- pres[! pres$random %in% pointsToRemove[1:nToRemove], ] # thin. 
      dTable <- dTable[! random %in% pointsToRemove[1:nToRemove], ] # remove from distance table.   
      
      print(paste("thinned", length(nToRemove), "points"))
      # I think once this else part runs, it should be done. 
        
      }
        
      if(length(pres)<=nthin){break} # if we reach goal, break.
        
      }
        
  if(length(pres)<=nthin){break} # if we reach goal, break. 
    
  }
}

    
name<-"wispClean"
writeVector(pres,
        paste0(folderPath, name, "ThinTest.shp"),
        overwrite=TRUE)
    



##### Now we have a list of points that are within 32km of another point.
##### we have random ID's that link this table of distance to pres points in shapefile.  
##### now we cycle through randomID's for certain distances (starting with <1km) 
##### and randomly delete those from the pres shapefile based on their 'random' ID. 




```
